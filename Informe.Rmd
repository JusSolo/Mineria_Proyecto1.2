---
title: "Proyecto1"
author:
- "Juan Luis Solórzano (carnet: 201598)"
- - "Micaela Yataz (carnet: 18960)"
date: "2025-01-20"
output: pdf_document
---






```{r paquetes necesarios, message=FALSE,warning=FALSE, echo=FALSE}
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
library(dplyr)
library(flexclust)
```

```{r cargando archivo, echo=FALSE}
#datos <- read.csv('movies.csv')
datos <- read.csv('movies.csv', fileEncoding = "latin1")

summary(datos)
```



# 1. Clustering
## 1.1. Haga el preprocesamiento del dataset, explique qué variables no aportan información a la generación de grupos y por qué. Describa con qué variables calculará los grupos.
Como el algoritmo de k-medias necesitan de alguna medida de distancia, entre los datos, en una primera instancia vamos a tomas solo las variables numéricas y vamos a quitar el id por ser como el nombre de una película. Las variables que tomaremos en consideración son las siguientes:


```{r, echo=FALSE}

datos_num <- datos %>%
  select(where(~ is.integer(.) | is.numeric(.)))

datos_num <- datos_num[,2:11]
str(datos_num)

```
Luego los normalizamos y centramos para que todas las variables estén en una escala comparable. 

```{r, include=FALSE}
datos_num <- scale(datos_num)
summary(datos_num)
```


## 1.2. Analice la tendencia al agrupamiento usando el estadístico de Hopkings y la VAT (Visual Assessment of cluster Tendency). Esta última hágala si es posible, teniendo en cuenta las dimensiones del conjunto de datos. Discuta sus resultados e impresiones.

```{r hopkins, echo=FALSE}
set.seed(155)
estadisticoHop <- hopkins(datos_num)
```
El estadístico de Hopkings es de `r estadisticoHop ` que es lejano a 0.5, entonces los datos no son aleatorios. Sin embargo no haremos un VAT por ser difícil de visualizar e interpretar con 10 variables. 

## 1.3. Determine cuál es el número de grupos a formar más adecuado para los datos que está trabajando. Haga una gráfica de codo y explique la razón de la elección de la cantidad de clústeres con la que trabajará.

Para ello se usará el metodo de Codo

```{r, echo=FALSE, warning=FALSE, fig.width=6, fig.height=4, dpi=300}
set.seed(940) #739102
wss=0
for (i in 1:40) 
  wss[i] <- sum(kmeans(datos_num, centers=i)$withinss)

plot(1:40, wss, type="b", xlab="Número de grupos",  ylab="suma de cuadrados intragrupo")
```

Como a partir de 10 grupos en adelante la suma de cuadrados intragrupo no disminuye significativamente se elegirán 10 grupos. 

## 1.4. Utilice los algoritmos k-medias y clustering jerárquico para agrupar. Compare los resultados generados por cada uno.



```{r, echo=FALSE, kmedias}
 
set.seed(102)
kmeans_result <- kmeans(datos_num, centers=10)

```

```{r jerárquico, echo=FALSE}
#Matriz de distncias
D <- dist(datos_num)


hc<-hclust(D, method = "ward.D2") #Genera el clustering jerÃ¡rquico de los datos
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma


```
```{r, comparar agrupamientos}
plotcluster(datos_num,kmeans_result$cluster)

```





## 1.5. Determine la calidad del agrupamiento hecho por cada algoritmo con el método de la silueta. Discuta los resultados.



## 1.6. Interprete los grupos basado en el conocimiento que tiene de los datos. Recuerde investigar las medidas de tendencia central de las variables continuas y las tablas de frecuencia de las variables categóricas pertenecientes a cada grupo. Identifique hallazgos interesantes debido a las agrupaciones y describa para qué le podría servir.




